{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 패키지 설치 및 라이브러리 불러오기"
      ],
      "metadata": {
        "id": "r2PwHHQskPYs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwaLsiK0hq6D"
      },
      "outputs": [],
      "source": [
        "!pip install gcsfs==2023.10.0 datasets==2.17.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU transformers==4.38.0 accelerate==0.27.1 bitsandbytes==0.42.0 peft==0.8.2 trl==0.7.10"
      ],
      "metadata": {
        "id": "0En8xlcSt9kD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "import json\n",
        "import time\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "\n",
        "from huggingface_hub import notebook_login"
      ],
      "metadata": {
        "id": "JUaIimoBuBh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 데이터셋 로드"
      ],
      "metadata": {
        "id": "refnVxL_lMJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# 데이터셋 로드\n",
        "dataset = load_dataset(\"ChuGyouk/argilla-distilabel-math-preference-dpo-korean\")\n",
        "# 데이터셋의 구조 확인\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "Z6rumZ7ytz73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. 데이터 전처리 (프롬프트)"
      ],
      "metadata": {
        "id": "O7KyQoY1ke1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 'prompt' 필드 생성 함수\n",
        "def format_instruction_ko(example):\n",
        "    # input 없이 instruction_ko와 chosen_response_ko만 사용\n",
        "    text = f\"\"\"user\\n{example[\"instruction_ko\"]}\\nmodel\\n{example[\"chosen_response_ko\"]}\"\"\"\n",
        "    return {'prompt': text}\n",
        "\n",
        "# 데이터셋의 prompt 필드를 업데이트\n",
        "dataset = dataset.map(format_instruction_ko)\n"
      ],
      "metadata": {
        "id": "604kkGGrt4Vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()"
      ],
      "metadata": {
        "id": "g1OeETYsuD2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. 모델 로드 및 양자화 설정"
      ],
      "metadata": {
        "id": "4DtqvbdYkxj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"google/gemma-2b\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(load_in_4bit=True,\n",
        "                                bnb_4bit_quant_type=\"nf4\",\n",
        "                                bnb_4bit_compute_dtype=torch.bfloat16)\n",
        "\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                             quantization_config=bnb_config,\n",
        "                                             device_map={\"\":0})\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)"
      ],
      "metadata": {
        "id": "8tVjNFCAVO5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. 데이터셋 토크나이징 및 학습/검증 세트 분할"
      ],
      "metadata": {
        "id": "9TD8e5eVk_M9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "6-W4AQXuuILm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)\n",
        "dataset = dataset['train'].train_test_split(test_size=0.2)\n",
        "dataset"
      ],
      "metadata": {
        "id": "NIxZzhmVuKwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = dataset[\"train\"]\n",
        "test_data = dataset[\"test\"]"
      ],
      "metadata": {
        "id": "rHsDPxIUuLQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 예측 모델을 위한 함수 정의"
      ],
      "metadata": {
        "id": "zrQuvB1DlilF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(query: str, model, tokenizer):\n",
        "\n",
        "  prompt_template = \"\"\"user\n",
        "  {query}\n",
        "\n",
        "  model\n",
        "  \"\"\"\n",
        "  prompt = prompt_template.format(query=query)\n",
        "  encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
        "  model_inputs = encodeds.to(\"cuda:0\")\n",
        "  generated_ids = model.generate(**model_inputs, max_new_tokens=256)\n",
        "  decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "  return decoded\n",
        "\n",
        "# Fine tuning 이전\n",
        "result = get_completion(query=\"식을 단순화하세요: 2(x + y) - 3(2x - y). 깊게 숨을 들이쉬고, 단계별로 생각하여 정확한 답변을 제공하세요.\", model=model, tokenizer=tokenizer)\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "8zB65o7XuOlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tunning 진행"
      ],
      "metadata": {
        "id": "TydhKz93lp3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=32,\n",
        "    target_modules=['o_proj', 'q_proj', 'up_proj', 'v_proj', 'k_proj', 'down_proj', 'gate_proj'],\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=test_data,\n",
        "    dataset_text_field=\"prompt\",\n",
        "    peft_config=lora_config,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=10,\n",
        "        max_steps=1000,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=10,\n",
        "        output_dir=\"outputs\",\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "    ),\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "iHItQLjJuRdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 예측 확인"
      ],
      "metadata": {
        "id": "7UUPSTL9lxfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine tuning 이후\n",
        "result = get_completion(query=\"식을 단순화하세요: 2(x + y) - 3(2x - y). 깊게 숨을 들이쉬고, 단계별로 생각하여 정확한 답변을 제공하세요.\",\n",
        "                        model=trainer.model,\n",
        "                        tokenizer=tokenizer)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "qmQBqL-ouTBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 저장"
      ],
      "metadata": {
        "id": "PSkhV6MKl01T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_model = \"gemma-2b-math-korean-finetuned\"\n",
        "trainer.model.save_pretrained(new_model)"
      ],
      "metadata": {
        "id": "6ZUW87Y2j0Ut"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}